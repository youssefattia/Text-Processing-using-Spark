{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b938b90-ed21-43e5-a02d-a0c475faa960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.7)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2022.6.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.1.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fbf561-dc69-4800-b4af-69f7530f2f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, udf\n",
    "from operator import add\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bebe2c61-91d4-42e9-9872-d55b841c0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"cite\") \\\n",
    ".config(\"spark.memory.fraction\", 0.8) \\\n",
    ".config(\"spark.executor.memory\", \"14g\") \\\n",
    ".config(\"spark.driver.memory\", \"12g\")\\\n",
    ".config(\"spark.sql.shuffle.partitions\" , \"800\") \\\n",
    ".config(\"spark.driver.maxResultSize\",  0) \\\n",
    ".getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacad79-93d7-483a-9fe4-360e1365fa7f",
   "metadata": {},
   "source": [
    "# Loading Data onto Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c99fef-b918-40ab-b690-9a88792da4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading the schema as mentioned in the README \n",
    "papersSchema = StructType([\n",
    "    StructField(\"paper_id\", StringType(), False),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"journal\", StringType(), True),\n",
    "    StructField(\"book_title\", StringType(), True),\n",
    "    StructField(\"series\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"pages\", IntegerType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"number\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"postedat\", IntegerType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True)\n",
    "])\n",
    "# load csv data onto the RDD\n",
    "papersDf = spark.read.csv(\"papers.csv\", header = False, schema = papersSchema).select('paper_id', 'title','abstract')\n",
    "\n",
    "# loading the users usersLibraries schema as mentioned in the README\n",
    "usersLibrariesSchema = StructType([\n",
    "    StructField(\"user_hash_id\", StringType(), False),\n",
    "    StructField(\"user_library\", StringType(), False)\n",
    "])\n",
    "# load data into dataframe\n",
    "usersLibrariesDf = spark.read.csv(\"users_libraries.txt\", sep = \";\", header = False, schema = usersLibrariesSchema)\n",
    "usersLibrariesDf = usersLibrariesDf.select(usersLibrariesDf.user_hash_id, \n",
    "                                           F.split(usersLibrariesDf.user_library,',').alias('user_library_arr'))\n",
    "\n",
    "\n",
    "# load stop words into a python list\n",
    "stopWords = [line.rstrip('\\n') for line in open('stopwords_en.txt')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e460be5-7d5c-43e9-ac07-43c2fd483f40",
   "metadata": {},
   "source": [
    "# Vector representation for the papers\n",
    "Steps to generate \"important\" terms T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f74e50d-939a-475e-a695-32acc2839c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the title and the abstract of each paper - [paper_id, content]\n",
    "papersDf = papersDf.select(\"paper_id\", F.concat_ws(' ',papersDf.title, papersDf.abstract).alias(\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d804dcf-fabb-4fc7-968e-c8cca317c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "\n",
    "# initialize regex tokenizer to include all english alphabets and - and _\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"content\", outputCol=\"tokens\", pattern=\"[^A-Za-z-_]+\")\n",
    "\n",
    "# setting minimum token length to three to automatically filter out tokens with smaller lengths \n",
    "regexTokenizer.setMinTokenLength(3)\n",
    "\n",
    "# Before running the tokenizer, remove all - and _ \n",
    "papersDf = papersDf.select('paper_id', F.regexp_replace(papersDf.content, \"[-_]\", \"\").alias('content'))\n",
    "\n",
    "# apply tokenization - [paper_id, tokens]\n",
    "papersDf = regexTokenizer.transform(papersDf).drop(\"content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e70ad5d-b80c-4156-91de-4c3032af25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords \n",
    "\n",
    "# initialize stopwords remover with stopwords extracted from file\n",
    "stopwordsremover = StopWordsRemover(stopWords=stopWords, inputCol=\"tokens\", outputCol=\"words\")\n",
    "# apply stopwords remover on paperDf - [paper_id, words]\n",
    "papersDf = stopwordsremover.transform(papersDf).drop(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa3846c-ad04-4b2d-b43f-4fb9229de179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "\n",
    "# expand before stemming so that operation can be applied on a column directly\n",
    "papersDf = papersDf.select('paper_id', F.explode('words').alias('terms'))\n",
    "\n",
    "# initialize porter stemmer instance\n",
    "ps = PorterStemmer() \n",
    "\n",
    "# create a UDF to apply porter stemming on all the terms\n",
    "udf_stem = udf(lambda x: ps.stem(x))\n",
    "\n",
    "# apply the udf on the column - [paper_id, term]\n",
    "papersDf = papersDf.withColumn(\"term\", udf_stem(col(\"terms\"))).drop(\"terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7a758d-062f-4961-a3ca-e9672bbf945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing irrelevant words\n",
    "nPapers = papersDf.count()\n",
    "\n",
    "# removing words that appear in more tham 10% of the papers\n",
    "maxThreshold = np.floor(0.1*nPapers)\n",
    "# removing words that appear in less that 20 papers\n",
    "minThreshold = 20\n",
    "\n",
    "# count all papers for each term - [term, paper_counts]\n",
    "wordCountDf = papersDf.groupBy(\"term\").count().select('term', col('count').alias('paper_counts'))\n",
    "\n",
    "# filter out all the terms that do not satisfy the criterion\n",
    "wordCountDf = wordCountDf.filter((wordCountDf.paper_counts > minThreshold) & (wordCountDf.paper_counts < maxThreshold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9fec9b0-5222-42d5-9fe6-78a19d4ded83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting to only the top 1000 words\n",
    "\n",
    "# sort by paper counts and limit to 1000\n",
    "wordCountDf = wordCountDf.orderBy(wordCountDf.paper_counts.desc()).limit(1000)\n",
    "# join with papersDf to select only those terms that remain after the above steps - [paper_id, term]\n",
    "paperWordsDf = wordCountDf.join(papersDf, papersDf.term == wordCountDf.term).select('paper_id', wordCountDf.term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aad926-c1db-4ef1-bfa8-68c80713871a",
   "metadata": {},
   "source": [
    "### Bag of words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce36deb2-396f-4e7f-9c88-4f9994b9e73b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = Window.orderBy(F.monotonically_increasing_id())\n",
    "# create an index for words corresponding to row_number - 1\n",
    "wordIndexDf = wordCountDf.withColumn(\"term_index\", F.row_number().over(w)-1).drop('paper_counts')\n",
    "\n",
    "# join with paper words to link with paper_id - [paper_id, term, term_index]\n",
    "paperWordsDf = paperWordsDf.join(wordIndexDf, paperWordsDf.term == wordIndexDf.term).drop(wordIndexDf.term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b6e81e-b293-40aa-8aad-c416f7bfc6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# initialize CountVectorizer \n",
    "cv = CountVectorizer(inputCol=\"terms\", outputCol=\"TermFrequencyVector\")\n",
    "\n",
    "# collect paperWords by id into a list - [paper_id, terms]\n",
    "collectedPaperWordsDf = paperWordsDf.groupBy('paper_id').agg(F.collect_list(col('term')).alias('terms'))\n",
    "\n",
    "# fit the dataframe above to the CountVectorizer initialized above\n",
    "cvModel = cv.fit(collectedPaperWordsDf)\n",
    "\n",
    "# transform the dataframe according to the model to generate tf scores\n",
    "paperTfDf = cvModel.transform(collectedPaperWordsDf)\n",
    "\n",
    "# final output containing [paper_id, TermFrequencyVector]\n",
    "paperTfDf = paperTfDf.drop('terms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af9d2a-18dc-4985-92fc-0675322877c0",
   "metadata": {},
   "source": [
    "## TF-IDF representation of the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc58aeb6-c6f6-4187-987e-11739d4bee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "# initializing IDF instance\n",
    "idf = IDF(inputCol=\"TermFrequencyVector\", outputCol=\"IdfVector\")\n",
    "# fit idf to the term frequency Df\n",
    "idfModel = idf.fit(paperTfDf)\n",
    "# generate idf scores and save as paperTfIdf dataframe\n",
    "paperTfIdf = idfModel.transform(paperTfDf)\n",
    "\n",
    "# utility function to multiply two sparse vectors - return a sparse matrix \n",
    "def sparse_multiply(x,y):\n",
    "    res = np.multiply(x,y).tolist()\n",
    "    # arguments to form a sparse matrix representation\n",
    "    vec_args =  len(res), [i for i,x in enumerate(res) if x != 0], [x for x in res if x != 0] \n",
    "    return Vectors.sparse(*vec_args)\n",
    "\n",
    "# user defined function to multiply sparse matrices (tf and idf vectors)\n",
    "sparse_multiply_udf = udf(sparse_multiply, VectorUDT())\n",
    "\n",
    "# generate tfIdfVector by multiplying termfrequency (tf) with idf vector\n",
    "paperTfIdf = paperTfIdf.withColumn('TfIdfVector', sparse_multiply_udf(paperTfIdf.TermFrequencyVector, paperTfIdf.IdfVector))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c34557-b930-4a43-a915-413b9dcb22f5",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "706377c2-52dc-4b68-ada9-877da231591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# initializing LDA instance with Expectation-Maximization as optimizer\n",
    "lda = LDA(k=40, seed=1, optimizer=\"em\", maxIter=5, featuresCol='TermFrequencyVector')\n",
    "# fit the \n",
    "ldaModel = lda.fit(paperTfDf)\n",
    "papersLdaDf = ldaModel.transform(paperTfDf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe topics - returns indices of terms corresponding to different topics\n",
    "topicsDescription = ldaModel.describeTopics(3)\n",
    "\n",
    "# initialize vocabulary from the cvModel\n",
    "vocab = cvModel.vocabulary\n",
    "\n",
    "# function to convert term indices to words from the vocabulary\n",
    "def indices_to_terms(termIndices):\n",
    "    return [vocab[int(idx)] for idx in termIndices]\n",
    "\n",
    "# user defined function to convert indices to words\n",
    "indices_to_terms_udf = udf(indices_to_terms, ArrayType(StringType()))\n",
    "\n",
    "# add column terms containing the words correspoding to the termIndices\n",
    "topicsDescription = topicsDescription.withColumn(\"terms\", indices_to_terms_udf(topicsDescription.termIndices))\n",
    "\n",
    "print(\"top five terms for each topic:\")\n",
    "# we only neede topic and terms columns - drop others\n",
    "topFiveTerms = topicsDescription.drop('termIndices', 'termWeights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e75b0-3340-4641-bbfa-60eb3aa913f5",
   "metadata": {},
   "source": [
    "## User profiling\n",
    "\n",
    "For calculating user profiles using 1. tf-idf scores and 2. LDA scores, we need to first need to write a utility function to calculate the sum of corresponding vectors. The tf-idf scores are in the form of sparse vectors, hence they muct be converted to a numpy array first and then a summation can be performed easily. The utility function must return a dense vector. This function is used as a udf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd3275da-5029-43b6-9d32-7d1c322c255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode is used to separate the paper_ids from the user_library_arr list into separate rows (opposite of groupBy) [user_hash_id, paper_id]\n",
    "usersPapersDf = usersLibrariesDf.select(usersLibrariesDf.user_hash_id, \n",
    "                                        F.explode(usersLibrariesDf.user_library_arr).alias('paper_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "458dec8b-1943-4aeb-b209-93f3dd43e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors\n",
    "\n",
    "# utility function to calculate the sum of vectors\n",
    "def sum_vectors(collectedTfIdfVector):\n",
    "    # If it is a list of SparseVector - convert to numpy array first\n",
    "    if type(collectedTfIdfVector[0]) is not list:\n",
    "        collectedVector = np.asarray([tfVector.toArray() for tfVector in collectedTfIdfVector])\n",
    "    else:\n",
    "        collectedVector = np.asarray(collectedTfIdfVector)\n",
    "    # Calculate the sum over the row axis and return as dense vector\n",
    "    return Vectors.dense(np.sum(collectedVector, axis=0))\n",
    "\n",
    "# user defined function to calculate the sum of all the tf-idf scores and return as a vector\n",
    "sumVectorUdf = udf(sum_vectors, VectorUDT())\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a480a-a811-480f-b1bc-2f036be88d26",
   "metadata": {},
   "source": [
    "### TF-IDF based profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31c52bb1-efb6-48c6-b2e9-993d2506fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute tf-idf profiles as the sum of all the tf-idf scores\n",
    "# the input Dataframe must be of the format [user_hash_id, paper_id]\n",
    "def compute_tf_idf_profiles(usersPapersDf):\n",
    "    # join with userPapersDf to formulate a mapping between user_hash_id and tf-idf scores\n",
    "    usersTfIdf = usersPapersDf.join(paperTfIdf,  usersPapersDf.paper_id == paperTfIdf.paper_id).drop(paperTfIdf.paper_id)\n",
    "    \n",
    "    # collect the tfIdf vectors corresponding to a user as an array of sparse vectors\n",
    "    usersTfIdf = usersTfIdf.groupBy('user_hash_id').agg(F.collect_list('TfIdfVector').alias('collectedTfIdfVector'))\n",
    "    \n",
    "    # return the sum of all the collected sparse tf-idf vectors \n",
    "    return usersTfIdf.select('user_hash_id', sumVectorUdf('collectedTfIdfVector').alias('tfIdfProfile'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fa630-bd14-47fe-9471-4e1310bb3624",
   "metadata": {},
   "source": [
    "### LDA based profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a34d8592-55be-4e8c-986a-959177d24c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute lda profiles as the sum of all the tf-idf scores\n",
    "# the input Dataframe must be of the format [user_hash_id, paper_id]\n",
    "def compute_lda_profiles(usersPapersDf):\n",
    "    # join with userPapersDf to formulate a mapping between user_hash_id and lda scores\n",
    "    usersLdaDf = usersPapersDf.join(papersLdaDf, papersLdaDf.paper_id == usersPapersDf.paper_id).drop(papersLdaDf.paper_id)\n",
    "    \n",
    "    # collect the lda vectors corresponding to a user as an array of lda vectors\n",
    "    usersLdaDf = usersLdaDf.groupBy('user_hash_id').agg(F.collect_list('topicDistribution').alias('collectedTopicDistribution'))\n",
    "    \n",
    "    # return the sum of all the collected lda vectors \n",
    "    return usersLdaDf.select('user_hash_id', sumVectorUdf('collectedTopicDistribution').alias('ldaProfile'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00628d2f-0e44-47ae-a92b-dafce1cc12bf",
   "metadata": {},
   "source": [
    "## Sampling and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e06f496d-f2fc-44b7-bf1a-b2b2bd06a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampler(n):\n",
    "    nUsers = usersLibrariesDf.count()\n",
    "    \n",
    "    # generate a sample containing 'n' users and their libraries \n",
    "    userLibrariesSampleDf = usersLibrariesDf.sample(fraction=n/nUsers, seed=1)\n",
    "    \n",
    "    # utility function to split the user_library corresponding to the user into train and test sets\n",
    "    def split_library(user_library):\n",
    "        lib_size = len(user_library)\n",
    "        # 80% of samples in the training set and 20% in the test set\n",
    "        split_idx = int(0.8*lib_size)\n",
    "        return user_library[0:split_idx], user_library[split_idx:lib_size]\n",
    "    \n",
    "    # user defined function to split a library into two two arrays containing train and test samples \n",
    "    splitUdf = udf(split_library, ArrayType(ArrayType(StringType())))\n",
    "    \n",
    "    # split all the user libraries into train and test set using the above udf \n",
    "    userLibrariesSampleDf = userLibrariesSampleDf.withColumn('user_library_split', splitUdf(userLibrariesSampleDf.user_library_arr))\n",
    "    \n",
    "    # separate the train library as the array on the first index in the split user library - [user_hash_id, train_library]\n",
    "    trainUserLibrarySampleDf = userLibrariesSampleDf.select('user_hash_id',\n",
    "                                                            userLibrariesSampleDf.user_library_split[0].alias('train_library'))\n",
    "    \n",
    "    # separate the test library as the array on the second index in the split user library - [user_hash_id, test_library]\n",
    "    testUserLibrarySampleDf = userLibrariesSampleDf.select('user_hash_id',\n",
    "                                                           userLibrariesSampleDf.user_library_split[1].alias('test_library'))\n",
    "    \n",
    "    # computing the tf-idf and lda profiles for th training samples\n",
    "    # first explode the train_library into separate paper_ids - [user_hash_id, paper_id]\n",
    "    explodedTrainUserLibrarySampleDf = trainUserLibrarySampleDf.withColumn('paper_id', \n",
    "                                                                           F.explode(trainUserLibrarySampleDf.train_library))\n",
    "    \n",
    "    # generate tf-idf profiles\n",
    "    tfIdfProfiles = compute_tf_idf_profiles(explodedTrainUserLibrarySampleDf)\n",
    "    # generate lda profiles\n",
    "    ldaProfiles = compute_lda_profiles(explodedTrainUserLibrarySampleDf)\n",
    "    \n",
    "    return tfIdfProfiles, ldaProfiles\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
